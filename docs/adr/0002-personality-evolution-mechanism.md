# ADR-0002: 人格演化跨模块闭环机制

**Status**: Proposed
**Date**: 2026-02-16
**Deciders**: Product & Architecture Team

## Context

AI School 的核心研究目标之一是观察学生人格如何在校园环境中随经历演变。这要求系统不仅能初始化差异化人格，还能让人格在外部交互过程中**持续、缓慢、可追踪地**演化。

### 核心问题

> 性格如何通过外部交互演化？这属于哪一个模块？

经分析，人格演化**不单属于某一个模块**，而是一个跨越 M1（Agent 系统）、M3（演化引擎）、M4（记忆与成长）三大模块的闭环流程。需要明确定义这个闭环的运作机制和各模块的职责边界。

### 研究背景

| 研究发现 | 来源 | 影响 |
|---------|------|------|
| Prompt 可改变语言风格，但未必改变深层决策行为 | Social Alignment (2024) | 人格演化不能仅靠修改 Prompt |
| 训练方法显著优于 Prompt 方法 | BIG5-CHAT (2024) | 深层人格可能需要参数化表示 |
| 人格一致性与模型规模无关 | PersonaGym (2024) | 不能假设更大模型更好 |
| 安全对齐训练压制"坏性格"表达 | TRAIT (2024) | 人格空间覆盖可能不完整 |
| 网络拓扑 × 人格分布共同决定宏观行为 | NetworkGames (2025) | 人格需与社交网络协同设计 |

## Decision Drivers

- 人格演化需要**可量化追踪**（支持研究目标）
- 演化速度需要**缓慢且可控**（人格不应在几次交互后剧变）
- 需要区分**"记住了新经历"与"人格真的变了"**两件不同的事
- 机制需要对 Social Alignment 警示的**表层 vs 深层**问题有应对

## Considered Options

### Option 1: 纯 Prompt 演化

通过动态修改 Agent 的 system prompt 来反映人格变化。

- **Pros**: 实现简单，无需额外基础设施
- **Cons**: Social Alignment 研究警示这可能只改变表层语言风格；难以量化追踪；无法保证长期一致性

### Option 2: 参数化人格空间 + 反思驱动演化（推荐）

维护一个显式的人格参数空间（如 MBTI 4 维连续分数），交互经历通过反思机制产生微调信号，缓慢推动参数变化。

- **Pros**: 可量化追踪，演化速率可控，区分记忆与人格，支持研究分析
- **Cons**: 需要设计反思→参数映射机制，增加系统复杂度

### Option 3: 训练方法（微调 Agent 模型）

通过累积经历数据对 Agent 模型进行周期性微调。

- **Pros**: 深层行为改变，BIG5-CHAT 证明训练方法优于 Prompt
- **Cons**: 成本极高，每个 Agent 需独立训练，不适合仿真规模

## Decision

采用 **Option 2：参数化人格空间 + 反思驱动演化**，定义以下跨模块闭环：

### 人格演化闭环

```
M2 (学校世界) 提供交互场景
       ↓
M1.3 (认知框架) 基于当前人格参数做出决策 → 产生行为
       ↓
M3.1 (演化循环) 行为与其他 Agent / 环境交互 → 产生结果与反馈
       ↓
M4.1 (多层记忆) 将经历存储 → 累积超过阈值触发反思
       ↓
M4.3 (人格演变) 反思结果 → 评估是否产生人格微调信号 → 更新人格参数
       ↓
反馈回 M1.3 → 更新后的人格参数影响下一次决策
```

### 各模块在闭环中的职责

| 模块 | 闭环职责 | 输入 | 输出 |
|------|---------|------|------|
| **M1.3 认知框架** | 基于人格参数生成行为决策 | 人格参数 Θ + 记忆 + 情境 | 行为意图（自然语言） |
| **M3.1 演化循环** | 执行交互、收集反馈 | 行为意图 | 交互结果、环境反馈 |
| **M4.1 多层记忆** | 存储经历、触发反思 | 交互结果 | 反思结论 |
| **M4.3 人格演变** | 评估反思结论，生成人格微调信号 | 反思结论 | 人格参数变化 ΔΘ |

### 人格参数空间设计

```
人格参数 Θ = {
    // MBTI 4 维连续分数（-1.0 ~ +1.0）
    E_I: float,     // 外倾 ←→ 内倾
    S_N: float,     // 感知 ←→ 直觉
    T_F: float,     // 思考 ←→ 情感
    J_P: float,     // 判断 ←→ 知觉

    // 演化元数据
    stability: float,       // 人格稳定度（随年龄/经历增长）
    last_shift_event: str,  // 最近一次人格微调的触发事件
    shift_history: [...]    // 人格变化历史（用于研究追踪）
}
```

### 演化速率控制

```
ΔΘ_actual = ΔΘ_signal × (1 / stability) × decay_factor

其中：
- ΔΘ_signal: 反思机制输出的原始微调信号（通常很小，如 ±0.01~0.05）
- stability: 人格稳定度（越高越难改变，模拟年龄增长后人格趋稳）
- decay_factor: 衰减系数（防止频繁微调导致人格剧变）
```

## Rationale

1. **参数化表示**解决了"人格真的变了"vs"只是记住了新经历"的区分问题
2. **反思驱动**与 Generative Agents 的反思机制一致，利用已验证的机制
3. **速率控制**确保人格演化是缓慢的，符合心理学中人格相对稳定的共识
4. **shift_history** 支持研究目标——可以回溯任何时刻的人格状态

## Consequences

### Positive

- 人格演化可量化、可追踪、可回放
- 清晰的模块职责划分，避免"谁负责人格演化"的模糊地带
- 演化速率可调，支持不同研究场景（如加速演化观察极端情况）
- 与 M5 研究分析平台天然对接（直接可视化人格参数轨迹）

### Negative

- 反思→人格参数映射的设计具有主观性，需要心理学专业输入
- 参数化空间可能无法捕捉所有人格维度（MBTI 4 维是否足够？是否需要 Big Five？）
- 增加了系统复杂度和调试难度

### Risks

- **对齐陷阱**：LLM 的安全训练可能压制"负面"人格演化方向（如变得更内向/更冷漠）
  - 缓解：考虑 TRAIT 基准的发现，可能需要特殊处理
- **演化可信度**：参数化演化是否真的映射了"真实的人格变化"有待验证
  - 缓解：通过 PersonaGym 类基准进行一致性验证

## Related Decisions

- [ADR-0001](0001-system-architecture-overview.md): 整体架构（模块定义）
- [ADR-0003](0003-world-system-llm-interaction.md): 世界系统与 LLM 交互（提供交互场景）
- 待决 D2: 人格框架选择（MBTI vs MBTI+Big Five 混合）

## References

- Social Alignment (2024) — Prompt 方法的深层局限性
- BIG5-CHAT (2024) — 训练方法 vs Prompt 方法
- PersonaGym (2024) — 人格一致性评估
- TRAIT (2024) — 安全对齐对人格表达的影响
- NetworkGames (2025) — 人格 × 网络拓扑的宏观效应

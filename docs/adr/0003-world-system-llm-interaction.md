# ADR-0003: 学校世界系统与 LLM 交互架构

**Status**: Proposed
**Date**: 2026-02-16
**Deciders**: Product & Architecture Team

## Context

AI School 的 M2（学校世界系统）需要为学生 Agent 提供结构化的活动环境。核心挑战在于：

> 学校世界系统如何与大模型交互，同时支持可视化？

系统中存在两种本质不同的计算：
1. **结构化计算**：时间推进、空间位置、课程表、关系矩阵 — 这些是确定性的、可查询的
2. **生成式计算**：Agent 决策、对话、行为合理性判断 — 这些需要 LLM 的语言理解和生成能力

如何桥接这两种计算，直接决定了系统的可靠性、可视化能力和成本效率。

### 研究背景

Concordia (Google DeepMind, 2023) 提出的 Game Master 机制为此问题提供了最直接的参考：

- 借鉴桌游中的"游戏主持人"概念
- 智能体用自然语言描述意图 → GM 翻译为环境变化
- 支持物理世界（验证合理性）、数字环境（API 调用）、社会环境（博弈）

Smallville (Park et al., 2023) 的虚拟小镇也提供了时空系统的设计参考，但其环境系统与 LLM 的耦合度较高，可视化依赖于游戏引擎。

## Decision Drivers

- **可视化优先**：研究平台的核心价值在于观察和分析，M5 需要直接读取世界状态
- **成本效率**：不是所有操作都需要 LLM 调用，结构化计算应由传统代码处理
- **可追踪性**：世界状态的每一次变化都需要可记录、可回放
- **一致性保障**：LLM 生成的行为需要经过合理性验证，不能直接生效

## Considered Options

### Option 1: 纯 LLM 驱动

所有世界交互通过 LLM 自然语言处理，世界状态隐含在对话历史中。

- **Pros**: 实现简单，灵活性高
- **Cons**: 不可查询，无法可视化，成本极高，状态一致性无法保障

### Option 2: 结构化世界状态 + Game Master 仲裁层（推荐）

世界状态用结构化数据维护，LLM 只在两个特定点被调用：Agent 决策和 Game Master 仲裁。

- **Pros**: 可视化直接读取结构化数据，成本可控，状态一致性有保障
- **Cons**: 需要设计 Game Master 仲裁逻辑，自然语言→结构化数据的转译增加复杂度

### Option 3: 事件溯源 + LLM 处理器

所有交互以事件流形式存储，LLM 作为事件处理器参与。

- **Pros**: 完整的历史可追溯，支持回放
- **Cons**: 事件设计复杂，仍需解决 LLM 输出到结构化状态的映射

## Decision

采用 **Option 2：结构化世界状态 + Game Master 仲裁层**，设计以下双层架构：

### 整体架构

```
┌─────────────────────────────────────────────────┐
│                可视化层 (M5.1)                    │
│  时间线 · 关系图谱 · 状态面板 · 事件回放          │
└──────────────────────┬──────────────────────────┘
                       ↑ 读取（直接查询结构化数据）
┌──────────────────────┴──────────────────────────┐
│          结构化世界状态管理器 (M2 核心)            │
│                                                  │
│  ┌──────────┐ ┌──────────┐ ┌──────────────────┐ │
│  │ 时间系统  │ │ 空间系统  │ │ 关系矩阵         │ │
│  │ 学期/日历 │ │ 校园地图  │ │ Agent间亲密度    │ │
│  │ 时间步进  │ │ 位置追踪  │ │ 群组/社团归属    │ │
│  └──────────┘ └──────────┘ └──────────────────┘ │
│  ┌──────────┐ ┌──────────┐ ┌──────────────────┐ │
│  │ 课程表   │ │ 事件队列  │ │ Agent 状态表     │ │
│  │ 学科/难度 │ │ 待处理事件│ │ 情绪/能力/人格   │ │
│  └──────────┘ └──────────┘ └──────────────────┘ │
│                                                  │
│  ← 所有状态可查询、可快照、可回放                  │
└──────┬───────────────────────────────┬──────────┘
       ↓ 提供情境（结构化→自然语言）     ↑ 回写状态变更（验证后）
┌──────┴──────┐                 ┌──────┴──────┐
│ Game Master  │ ← 仲裁协调 → │  Agent LLM   │
│ (世界裁判)    │                │  (学生决策)   │
│              │                │              │
│ 职责：        │                │ 职责：        │
│ · 验证行为    │                │ · 感知情境    │
│   合理性      │                │ · 思考决策    │
│ · 仲裁交互    │                │ · 输出行为    │
│   结果        │                │   意图        │
│ · 翻译自然    │                │              │
│   语言→结构   │                │ 输入：        │
│   化状态变更  │                │ 人格参数 Θ    │
│              │                │ + 检索记忆    │
│ ← LLM 调用   │                │ + 当前情境    │
│   点 #2      │                │              │
└─────────────┘                 │ ← LLM 调用   │
                                │   点 #1      │
                                └─────────────┘
```

### LLM 调用点设计

系统中 LLM **仅在两个明确的点**被调用，其余全部由传统代码处理：

| 调用点 | 触发时机 | 输入 | 输出 | 调用频率 |
|--------|---------|------|------|---------|
| **#1 Agent 决策** | 每个 Agent 的行动轮次 | 人格参数 + 检索记忆 + 情境描述 | 行为意图（自然语言） | 高频 |
| **#2 Game Master 仲裁** | Agent 行为涉及环境/他人交互时 | Agent 行为意图 + 世界当前状态 + 规则约束 | 验证结果 + 结构化状态变更指令 | 中频 |

### 数据流示例

以"小明在数学课上与小红因分组课题发生争执"为例：

```
[时间系统] 推进到 09:00 → 触发"数学课开始"
     ↓
[课程系统] 查询：小明和小红都在数学课 → 生成情境描述
     ↓
[Agent LLM #1] 输入：小明的人格(T偏高) + 记忆(上次合作不愉快) + 情境(被分到同组)
              输出："我想按照我的方案来，如果小红不同意就据理力争"
     ↓
[Agent LLM #1] 输入：小红的人格(F偏高) + 记忆(小明上次态度强硬) + 情境(被分到同组)
              输出："我希望能好好沟通，但如果小明又很固执我会很难过"
     ↓
[Game Master #2] 输入：两人行为意图 + 当前关系状态(亲密度=0.4)
                输出：{
                  event_type: "conflict",
                  intensity: 0.6,
                  state_changes: [
                    { target: "小明.emotion", delta: -0.2 },
                    { target: "小红.emotion", delta: -0.3 },
                    { target: "relationship[小明,小红].closeness", delta: -0.15 }
                  ],
                  narrative: "两人因方案分歧发生争执，小明坚持逻辑分析，小红感到不被尊重"
                }
     ↓
[世界状态管理器] 验证变更合法性 → 执行状态更新 → 记录事件日志
     ↓
[M4.1 记忆] 将事件分别写入小明和小红的记忆
     ↓
[M5.1 可视化] 实时更新展示
```

### Game Master 的职责边界

Game Master **不是**全能的叙事引擎，它有明确的职责边界：

| GM 负责 | GM 不负责 |
|---------|----------|
| 验证行为在物理上是否合理（不能穿墙） | Agent 的决策过程（由 Agent LLM 负责） |
| 仲裁多 Agent 交互的结果（谁赢了辩论） | 创造新的世界规则 |
| 将自然语言翻译为结构化状态变更 | 生成长篇叙事（由可视化层负责展示） |
| 检测是否触发特殊事件（如友谊破裂阈值） | 人格参数的变化（由 M4.3 负责） |

## Rationale

1. **结构化世界状态**使得可视化可以直接查询数据，无需解析 LLM 输出
2. **两个 LLM 调用点**的设计最大化了成本效率——时间推进、位置移动、课程表等不需要 LLM
3. **Game Master 仲裁层**解决了 LLM 输出到结构化数据的桥接问题，参考 Concordia 已验证的模式
4. **状态变更以结构化指令形式**而非自然语言描述，确保了可追踪性和可回放性

## Consequences

### Positive

- 可视化层直接读取结构化数据，无需 NLP 解析，响应速度快
- LLM 成本可控——大量结构化计算由传统代码处理
- 世界状态可快照、可回放、可对比（支持 M5.3 实验对照）
- Game Master 作为"安全阀"，防止 LLM 产生不合理的世界状态变更

### Negative

- Game Master 本身也是 LLM 调用，增加了一层延迟和成本
- 自然语言→结构化状态变更的翻译可能引入错误（需要 schema 验证）
- 结构化世界状态的 schema 设计需要预先规划，灵活性低于纯文本

### Risks

- **GM 瓶颈**：所有 Agent 交互都经过 GM 可能成为性能瓶颈
  - 缓解：非交互行为（如独自学习）可跳过 GM，仅涉及多 Agent 或环境变更时调用
- **状态一致性**：并发 Agent 的状态变更可能冲突
  - 缓解：引入乐观锁或事件队列序列化处理

## Implementation Notes

- 世界状态管理器建议使用关系型数据库（PostgreSQL）存储，支持复杂查询
- Game Master 的状态变更输出应符合预定义的 JSON Schema，便于验证
- 考虑为非交互行为设计"快速通道"，跳过 GM 直接更新状态

## Related Decisions

- [ADR-0001](0001-system-architecture-overview.md): 整体架构
- [ADR-0002](0002-personality-evolution-mechanism.md): 人格演化闭环（Agent LLM 的输入之一）
- [ADR-0004](0004-narrative-data-dual-view.md): 叙事-数据双视图（可视化层的设计原则）
- 待决 D1: LLM 选型（直接影响 Agent LLM 和 GM 的实现）
- 待决 D4: 仿真规模（影响 GM 的并发设计）

## References

- Concordia (Google DeepMind, 2023) — Game Master 机制的原始设计
- Generative Agents (Park et al., 2023) — 虚拟小镇的时空系统
- AgentSociety (2025) — 10,000+ Agent 的大规模仿真经验
